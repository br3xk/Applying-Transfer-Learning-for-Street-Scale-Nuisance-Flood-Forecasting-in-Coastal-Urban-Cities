# Applying-Transfer-Learning-for-Street-Scale-Nuisance-Flood-Forecasting-in-Coastal-Urban-Cities
We apply Transfer Learning for Street-Scale Nuisance Flood Forecasting in Coastal-Urban Cities using an LSTM model. This is implemented in Keras Python.

**Background**

This study investigates the use of Transfer Learning (TL) for street-scale nuisance flood forecasting by exploring whether an ML model trained on data collected for one set of streets can effectively forecast flooding for another set of streets in the same city using TL. The envisioned use case is a city deploying a new flood depth sensor on a street and using TL to apply a ML model, trained on sensor data from an existing flood depth sensor network, to this new street. This method is explored using a Long-Short Term Memory (LSTM) model trained on data for the flood-prone streets of Norfolk City, Virginia. 

**Study Area** 

The City of Norfolk and its surrounding larger Hampton Roads region of Virginia, located on the U.S. East Coast is highly vulnerable to nuisance flooding. A portion of this city was selected as the study area, covering a total area of 47.70 km2. Within this area, the flood-prone streets were identified from the STORM flood report and they were randomly divided into source and target streets, with 180 streets for each, to build train the base and transfer models, respectively. The source and target streets were selected so that they remained mutually exclusive, i.e., the source street did not have any adjacent/overlapped target street and the target street did not have any adjacent /overlapped source street either. We applied TL within a small city, so though we had different source and target streets, the storm events were the same in source and target domains.

**Input Data** 

The model's inputs include topographic features, such as the topographic wetness index, depth to water, and elevation, as well as environmental features like hourly rainfall, cumulative rainfall in previous hours, and hourly tide level. The input also includes hourly water depth on streets during storm events generated by the 1-D /2-D coupling hydrodynamic model TUFLOW. The input data is available on Hydroshare (Roy, B., 2025).

**Methodology** 

The TL techniques were adopted from K. Ma et al. (2021) and modified to suit our ML architecture and Keras framework. Based on the weight adjustment strategies, four TL techniques are applied in this study. Here, Options TL-a and TL-d served as the highest and lowest degrees of transfer learning, respectively.

- For option TL-a, weights (W) and bias (B) of the output linear Dense layer were allowed to be updated; the rest of the weights were frozen to prevent the values from changing. 
- For the other options (TL-b, TL-c, TL-d), the weights of the non-linear LSTM layers themselves were also allowed to be updated as follows.   
  - For TL-b, the weights of the recurrent connections from hidden states (recurrent weights U) were allowed to change, but the weights from inputs (input weights W) and shared bias (B) were frozen. 
  - For TL-c, the weights from inputs (input weights W) were allowed to change, but the weights of the recurrent connections from hidden states (recurrent weights U) and shared bias (B) were frozen.
  - For TL-d, all weights (W, U) and bias (B) in the network were allowed to be updated, nothing is frozen.

Here, Fig. 1a shows the architecture of the LSTM Model and Fig. 1b shows TL techniques implemented in Keras Python. In Keras LSTM, the weights of input and hidden state are split and handled separately as input weight (W) and recurrent weight (U), respectively, whereas the bias term (B) is shared by both input and hidden state. The output dense linear layer doesn’t have any hidden states, so it has no recurrent weights; rather, it has only output weights (W) and bias (B). 

![image](https://github.com/user-attachments/assets/96f0f73a-edd0-41db-aa20-0be30901755a)
Fig. 1. (a) Architecture of the LSTM model, x denotes the input features and y denotes the target features and (b) Architecture of the LSTM model with TL techniques implemented in Keras Python (techniques were modified from K. Ma et al. (2021), which had different input layer and was implemented in PyTorch).

Here, we introduce some definitions and notations related to transfer learning that are used throughout the paper. 
- The Base model is built on the source streets from scratch.
- The Local model is built on the target streets from scratch.
- The Transfer model is the Base model that is transferred to the target domain. There are two types:
  - wo TL model = test the Base model on the target streets, with no transfer learning weight adjustments, and
  - w TL model = re-train the Base model on the target streets, with different transfer learning weight adjustment strategies. 

The notations that have been used throughout the study are defined as follows.

E = total number of storm events
N = total number of street segments in Norfolk, Virginia
S = total number of flood-prone streets in Norfolk, Virginia; mathematically: S ⊆ N
SS = set of flood-prone street segments in the source domain used to train the base model; mathematically: SS ⊆ S
ST = set of flood-prone street segments in the target domain used to train the local model or re-train the transfer model; mathematically: ST ⊆ S
Here, Ss and ST are mutually exclusive, i.e., no street element is common between source and target domain, mathematically: Ss ∩ ST =∅
ES = set of storm events in the source domain
ET = set of storm events in the target domain
Here, ES and ET are common between source and target domain, mathematically: ES = ET  = E

**Experiments/ Training Configurations** 

A total of 76 models were developed to evaluate the performance of TL under different training configurations. This includes 64 TL models generated from combinations of four TL techniques (TL-a, TL-b, TL-c, TL-d), four sets of source streets (10 %,  20 %, 60 %, and 100 % streets), and four sets of target events (10 %,  20 %, 60 %, and 100 % events), resulting in 4 × 4 × 4 = 64. In addition, we included 4 base models trained only on source streets (10 %,  20 %, 60 %, and 100 % streets), 4 local models trained only on target events (10 %, 20 %, 60 %, and 100 % events) and 4 models w/o TL, where base model were applied directly to target data without fine-tuning. So, there were 76 models in total. 


References: 

Roy, B. (2025). Input Data for LSTM Model for Street-Scale Nuisance Flood Forecasting in Norfolk, Virginia using Transfer Learning, HydroShare, http://www.hydroshare.org/resource/cdaaadf3e934466a85327a9c3ee1f3e0
